# -*- coding: utf-8 -*-
"""DriftModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jcx8cXrFa7J2wwTNDhsCMIPh-Nw318eR
"""

import numpy as np
import pandas as pd
import random
import seaborn as sns
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.parameter import Parameter
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.metrics import accuracy_score, balanced_accuracy_score
import scipy.io
import torch

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv('Normal Data')
df=df.drop(df.columns[0], axis=1)
labels=list(df.iloc[:,-1])
labels=pd.DataFrame({'labels':labels})

df=df.drop(df.columns[3],axis=1)

'''import seaborn as sns

sns.distplot(df['x'], hist=True, kde=True, 
             bins=int(len(df)/20000), color = 'darkblue', 
             hist_kws={'edgecolor':'black'},
             kde_kws={'linewidth': 4})
# seaborn histogram

# Add labels
plt.title('Histogram of sensor-data')
plt.xlabel('interval')
plt.ylabel('x-values')'''

'''
df=pd.read_csv('/content/drive/MyDrive/motor.csv')
df=df.drop(df.columns[0],axis=1)
df=df.rename(columns={'0':'x','1':'y','2':'z','0.1':'label'})
labels=df.iloc[:,-1]
labels=pd.DataFrame({'labels':labels})
df=df.drop(df.columns[3],axis=1)
'''

#from sklearn.preprocessing import OneHotEncoder
#from sklearn.compose import ColumnTransformer
'''
onehotencoder = OneHotEncoder()
onehotencoder.fit(labels)
onehotlabels = onehotencoder.transform(labels).toarray()
labels=pd.DataFrame(onehotlabels)
labels.reset_index(drop=True,inplace=True)
df.reset_index(drop=True,inplace=True)
data=pd.concat([df,labels],axis=1)
'''





X_train, X_test, y_train, y_test = train_test_split(df,labels, test_size=0.2, random_state=42, shuffle=True)
#X_train, X_test, y_train, y_test = train_test_split(df, labels, test_size=0.2, random_state=42, shuffle=True)

X_train=np.asarray(X_train)

y_train=np.asarray(y_train)
X_test=np.asarray(X_test)
y_test=np.asarray(y_test)





'''
fig=plt.figure(figsize=(8,8))
ax=Axes3D(fig)
ax.scatter(X_train[:1000,0],X_train[:1000,1],X_train[:1000,2],c=y_train[:1000])
#fig.savefig('figure')
'''

X_train, X_test, y_train, y_test = map(torch.tensor,(X_train, X_test, y_train, y_test))

class model(nn.Module):

  def __init__(self,features,neurons,max_hidden_layers,classes,use_cuda=False):
    super(model,self).__init__()
    
    if torch.cuda.is_available() and use_cuda:
        print("Using CUDA :]")

    self.device = torch.device(
        "cuda:0" if torch.cuda.is_available() and use_cuda else "cpu")

    self.features=features
    self.neurons=neurons
    self.max_hidden_layers=max_hidden_layers
    self.classes=classes
    
    self.hidden_layers = []
    self.output_layers = []
   
    self.b = Parameter(torch.tensor(0.99), requires_grad=False).to(self.device)
    
    self.n = Parameter(torch.tensor(
          0.4), requires_grad=False).to(self.device)
    self.s = Parameter(torch.tensor(
            0.2), requires_grad=False).to(self.device)


    self.hidden_layers.append(nn.Linear(features,neurons,bias=False))
   
    for i in range(max_hidden_layers-1):
      self.hidden_layers.append(nn.Linear(neurons,neurons,bias=False))

    for i in range(max_hidden_layers):
      self.output_layers.append(nn.Linear(neurons,classes,bias=False))

    
    self.hidden_layers=nn.ModuleList(self.hidden_layers).to(self.device)
    self.output_layers=nn.ModuleList(self.output_layers).to(self.device)

    self.sigmoid = nn.Sigmoid()

    self.loss_array=[]
   
    self.a=Parameter(torch.Tensor(self.max_hidden_layers).fill_(1/self.max_hidden_layers+1),requires_grad=False)
    print(self.a)

    
 
  def zero_grad(self):

    for i in range(self.max_hidden_layers):
        
      self.output_layers[i].weight.grad.data.fill_(0)
      self.hidden_layers[i].weight.grad.data.fill_(0)
      


    #UPDATE FUNCTION

  def update(self,x,y):

    predictions=self.forward(x) #GIVES PREDICTION VALUE FOR EACH HIDDEN LAYER 

    losses=[]

    for prediction in predictions:
      
      prediction=prediction.reshape((1,self.classes))
      

      criterion=nn.CrossEntropyLoss().to(self.device)

      
      loss=criterion(prediction,y)  
      
      
      losses.append(loss) #EACH LAYER LOSSES ARE STORED IN A LIST



 
                        #UPDATING THE WEIGHTS OF EACH OUTPUT LAYER..
                        #EACH HIDDEN LAYER HAS OUTPUT LAYER.. SO UPDATING THE WEIGHTS OF EACH OUTPUT LAYER IS SAME AS IN TRADITIONAL DEEP LEARNING...

    
    
    w=[None]*(self.max_hidden_layers) #WEIGHTS MATRICES OF EACH HIDDEN LAYERS STORE IN A LIST
    
    with torch.no_grad():
      
      for i in range((self.max_hidden_layers)):
  
        losses[i].backward(retain_graph=True) #EACH LAYER LOSS FUNCTION IS BACKPROPAGATED AND GRAPH RETAIN ALL THE PARAMETER
  
        self.output_layers[i].weight.data= -self.n * self.a[i] * self.output_layers[i].weight.grad.data
    
#UPDATING THE WEIGHTS OF EACH HIDDEN LAYER....EACH HIDDEN LAYER WEIGHT MATRIX DEPENDS ON EACH OTHER    

      for i in range(self.max_hidden_layers): #WEIGHT UPDATION OF LAYER 1 IS FROM 1 TO L...OF LAYER 2 IS FROM 2 TO L...3...L.....
                                              #BACKPROPAGATION OF EACH HIDDEN LAYER IS DONE SEPARATELY 
        for j in range(i+1):

          if w[j] is None:

            w[j]= self.a[i] * self.hidden_layers[j].weight.grad.data # EACH HIDDEN LAYER IS REWARDED BY ITS REWARD VALUE 'a'.
          else:                                                      #'a' VALUE IS UNIFORMALY DISTRIBUTED FOR EACH LAYER.

            w[j]+= self.a[i] * self.hidden_layers[j].weight.grad.data
      
      for i in range(self.max_hidden_layers):    

        self.hidden_layers[i].weight.data= -self.n * w[i]        #FINAL UPDATION OF EACH HIDDEN LAYER WEIGHT MATRIX.
        
      for i in range(self.max_hidden_layers):

        self.a[i] *= torch.pow(self.b,losses[i])                #REWARD FACTOR IS DYNAMICALLY VARYING WITH LOSSES OCCURED DUE TO EACH INSTANCE

      #  self.a[i] = torch.max(self.a[i], self.s / self.max_hidden_layers) #FOR STABLE OPERATION OF REWARD FACTOR SO THAT IT DOESNT AFFECT WEIGHT UPDATION.
                                                       #STABLE FACTOR 's' IS ALSO 0 TO 1.
    self.a=Parameter(self.a/(torch.sum(self.a)),requires_grad=False).to(self.device)           #NORMALISE REWARD FACTOR 
    

    Prediction=torch.sum(torch.mul(self.a.view(self.max_hidden_layers, 1),predictions), 0)
    
    criterion = nn.CrossEntropyLoss().to(self.device)
    loss = criterion(Prediction.view(1, self.classes),y.view(1).long())
    self.loss_array.append(loss)
    if (len(self.loss_array) % 1000) == 0:
      
      loss = torch.Tensor(self.loss_array).mean().cpu().numpy()
      print("Alpha:" + str(self.a.data.cpu().numpy()))
      print("Training Loss: " + str(loss))   
      self.loss_array.clear()


           #FEED FORWARD OF EACH LAYER AND OUPUT PREDICTION OF EACH LAYER
  
  def forward(self,x):

    hidden_connections = [] #ASSIGNING ALL THE HIDDEN LAYER TO HIDDEN LAYER OUTPUT.
 
    X=self.sigmoid(self.hidden_layers[0](x)).to(self.device) #FIRST HIDDEN LAYER SIGMOID OUTPUT TO NEXT HIDDEN LAYER.
  
    hidden_connections.append(X)

    for i in range(1,self.max_hidden_layers):

      hidden_connections.append(torch.sigmoid(self.hidden_layers[i](hidden_connections[i-1])))

    output_classes=[]
    for i in range(self.max_hidden_layers):
    
      output_classes.append(self.output_layers[i](hidden_connections[i]))

    
    predictions=torch.stack(output_classes) #PREDICTION OF EACH HIDDEN LAYER IS STORED IN A STACK
   # print(predictions.view(20,1))
    return predictions 
#Model is preparing...

  def fit(self,x,y):

    self.update(x,y) 


  def predict(self,X):
    
    #return torch.argmax(torch.sum(torch.mul(self.a.view(self.max_hidden_layers,1),self.forward(X)),0),dim=0)


    predict=torch.argmax(torch.sum(torch.mul(self.a.view(self.max_hidden_layers,1).repeat(1,len(X)).view(self.max_hidden_layers,len(X),1),self.forward(X)),0),1)

    return predict

path='Anomaly data'
df=pd.read_csv(path)
df=df.drop(columns='Unnamed: 0')
y=pd.DataFrame(df.iloc[:,-1])
df=df.drop(df.columns[3],axis=1)
x_t=torch.tensor(df.values)
y_t=torch.tensor(y.values)

m=model(features=3,neurons=20,max_hidden_layers=4,classes=4) #CLASS MODEL-> OBJECT IS DEFINED
m.double()#DOUBLE FLOAT THE PARAMETERS OF MODEL

#MODEL TRAINING.... WITH EACH INSTANCE IS COMING FROM TRAINING BATCH.
from sklearn.metrics import accuracy_score, balanced_accuracy_score

Acc=[]
Loss=[]


#for i in range(10):
#y_train=y_train.type(torch.LongTensor)

j=0

for i in range(100000):

 #print('\nfor instance {}...\n'.format(i+1))  

  m.fit(X_train[i],y_train[i])

  if (i%1000==0):
    print("Training examples {} are processed".format(i))

    predictions=m.predict(x_t)        
    Acc.append((accuracy_score(y_t, predictions))) 


    print("\nOnline Accuracy: {}\n".format(accuracy_score(y_t, predictions)))




    alpha_w = 0.05;
    alpha_d = 0.04;
    alpha   = 0.0001;
    dt1=predictions
    acc=[]
    for i in range(len(dt1)):
      if dt1[i]==y_t[i]:
        acc.append(0)
      else:
        acc.append(1)

    cuttingpoint = 0
    pp = len(dt1)
    F_cut = acc
    Fupper = np.max(F_cut)
    Flower = np.min(F_cut)
    miu_F = np.mean(F_cut)

    for idx in range(pp):
      cut = idx + 1
      miu_G = np.mean(F_cut[0:cut])
      Gupper = np.max(F_cut[0:cut])
      Glower = np.min(F_cut[0:cut])
      epsilon_G = (Gupper - Glower) * np.sqrt(((pp)/(2*cut*(pp)) * np.log(1/alpha)))
      epsilon_F = (Fupper - Flower) * np.sqrt(((pp)/(2*cut*(pp)) * np.log(1/alpha)))

      if ((epsilon_G + miu_G) >= (miu_F + epsilon_F) and cut<pp):
          cuttingpoint = cut
          miu_H = np.mean(F_cut[(cuttingpoint):])
          epsilon_D = (Fupper - Flower) * np.sqrt(((pp-cuttingpoint)/(2*cuttingpoint*(pp-cuttingpoint)) * np.log(1/alpha_d)))
          epsilon_W = (Fupper - Flower) * np.sqrt(((pp-cuttingpoint)/(2*cuttingpoint*(pp-cuttingpoint)) * np.log(1/alpha_w)))
      
          break

    if((np.abs(miu_G - miu_H)) > epsilon_D and cuttingpoint>1):
      print(miu_G,miu_H, epsilon_D )
      print("instance",cuttingpoint)
      print('Drift state: DRIFT\n')

    elif((np.abs(miu_G - miu_H)) >= epsilon_W and (np.abs(miu_G - miu_H)) < epsilon_D):
      print('Drift state: WARNING\n')

    else:
      print('Drift state: STABLE\n')



'''
x=np.asarray(x_t)

y_t=np.asarray(y_t)

fig=plt.figure(figsize=(8,8))
ax=Axes3D(fig)
ax.scatter(x_t[:1000,0],x_t[:1000,1],x_t[:1000,2],c=y_t[:1000])
fig.savefig('figure')
'''



import matplotlib.pyplot as plt
figure=plt.figure(figsize=(10,10))
epochs=range(0,100)
plt.plot(epochs,Acc,'g')
plt.xlabel("Sequencewise_Data")
plt.ylabel(" Testdata_Accuracy")
plt.title("Sensor-Data")



'''
test_y=pd.read_csv('/content/drive/MyDrive/SEA_data (1)/SEA_testing_class (1).csv')

test_x=pd.read_csv('/content/drive/MyDrive/SEA_data (1)/SEA_testing_data (1).csv')
train_y=pd.read_csv('/content/drive/MyDrive/SEA_data (1)/SEA_training_class (1).csv')
train_x=pd.read_csv('/content/drive/MyDrive/SEA_data (1)/SEA_training_data (1).csv')

train_x=np.asarray(train_x)

train_y=np.asarray(train_y)
test_x=np.asarray(test_x)
test_y=np.asarray(test_y)


test_y,test_x,train_y,train_x=map(torch.tensor,(test_y,test_x,train_y,train_x))

'''



#Code for SEA data set
'''
m=model(features=3,neurons=20,max_hidden_layers=4,classes=3) #CLASS MODEL-> OBJECT IS DEFINED
m.double()#DOUBLE FLOAT THE PARAMETERS OF MODEL

#MODEL TRAINING.... WITH EACH INSTANCE IS COMING FROM TRAINING BATCH.
from sklearn.metrics import accuracy_score, balanced_accuracy_score

Acc=[]
Loss=[]

j=0

for i in range(len(train_x)):

  #print('\nfor instance {}...\n'.format(i+1))
  #print('\nfor instance {}...\n'.format(i+1))
  m.fit(train_x[i],train_y[i])



  if (i%250==0 and i!=0):
  
    print("Training examples {} are processed".format(i))

    predictions=m.predict(test_x[j:i]) 

    

    Acc.append(accuracy_score(test_y[j:i], predictions))

    print("Online Accuracy: {}".format(accuracy_score(test_y[j:i], predictions)))
    
    j=j+250
    
    #print("{} Examples  are Tested\n".format(j))
  


    alpha_w = 0.05;
    alpha_d = 0.04;
    alpha   = 0.01;
    dt1=predictions
    acc=[]
    for i in range(len(dt1)):
      if dt1[i]==test_y[i]:
        acc.append(0)
      else:
        acc.append(1)

    cuttingpoint = 0
    pp = len(dt1)
    F_cut = acc
    Fupper = np.max(F_cut)
    Flower = np.min(F_cut)
    miu_F = np.mean(F_cut)

    for idx in range(pp):
      cut = idx + 1
      miu_G = np.mean(F_cut[0:cut])
      Gupper = np.max(F_cut[0:cut])
      Glower = np.min(F_cut[0:cut])
      epsilon_G = (Gupper - Glower) * np.sqrt(((pp)/(2*cut*(pp)) * np.log(1/alpha)))
      epsilon_F = (Fupper - Flower) * np.sqrt(((pp)/(2*cut*(pp)) * np.log(1/alpha)))

      if ((epsilon_G + miu_G) >= (miu_F + epsilon_F) and cut<pp):
          cuttingpoint = cut
          print("cut",cuttingpoint)
          miu_H = np.mean(F_cut[(cuttingpoint):])
          epsilon_D = (Fupper - Flower) * np.sqrt(((pp-cuttingpoint)/(2*cuttingpoint*(pp-cuttingpoint)) * np.log(1/alpha_d)))
          epsilon_W = (Fupper - Flower) * np.sqrt(((pp-cuttingpoint)/(2*cuttingpoint*(pp-cuttingpoint)) * np.log(1/alpha_w)))
          print((miu_G,miu_H),epsilon_D)
          print("\n")
          break

    if((np.abs(miu_G - miu_H)) > epsilon_D and cuttingpoint>1):
      print('Drift state: DRIFT\n')

    elif((np.abs(miu_G - miu_H)) >= epsilon_W and (np.abs(miu_G - miu_H)) < epsilon_D):
      print('Drift state: WARNING\n')

    else:
      print('Drift state: STABLE\n')


'''

#Hyperplane dataset
'''path='/content/drive/MyDrive/hyperplane.mat'
data=scipy.io.loadmat(path)
data=data.get('data')
data=pd.DataFrame(data)

labels=data.iloc[:,4]

L=pd.DataFrame(labels)
data=data.drop(columns=5,axis=1)
data=data.drop(columns=4,axis=1)

data'''

'''
import matplotlib.pyplot as plt
figure=plt.figure(figsize=(10,10))
epochs=range(0,199)
plt.plot(epochs,Acc,'g')
plt.xlabel("Sequencewise_Data")
plt.ylabel(" Testdata_Accuracy")
plt.title("Artificial-Data")
'''



